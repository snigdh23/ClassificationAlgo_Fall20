{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS AND USER INPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the filename: data2.txt\n",
      "Do you wish to input a sample (Y/N): n\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "\n",
    "filePath = input(\"Enter the filename: \")\n",
    "df = pd.read_csv(filePath, sep = \"\\t\", lineterminator = '\\n', header = None)\n",
    "        \n",
    "sample_need = input(\"Do you wish to input a sample (Y/N): \").lower()\n",
    "sample = \"\"\n",
    "upd_sample = \"\"\n",
    "if(sample_need==\"y\"):\n",
    "    sample = (input(\"Please enter the sample in the {a,b,c,d} format: \"))\n",
    "    upd_sample = sample[1:-1].split(\",\") \n",
    "#{rain,hot,high,weak}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_check(list1):\n",
    "    categ = True\n",
    "    for i in (list1):\n",
    "        if(i.isalpha() == False):\n",
    "            categ = False\n",
    "            break\n",
    "    return categ\n",
    "\n",
    "def demo_nb(X):\n",
    "    categ = feature_check(df.iloc[0,:-1].values.tolist()) and feature_check(X)\n",
    "    if(categ == False):\n",
    "        print(\"This branch is for categorical dataset and sample!\")\n",
    "    else:\n",
    "        items_list = {}\n",
    "        for i in range(df.shape[1]-1):\n",
    "            unique_labels12 = np.unique(df.iloc[:,i].values).tolist()\n",
    "            items_list[i] = unique_labels12\n",
    "            \n",
    "        prob_dict = {}\n",
    "        (unique_labels,counts) = np.unique(df.iloc[:,-1].values, return_counts = True)\n",
    "        for i in range(len(counts)):\n",
    "            prob_dict[unique_labels[i]] = counts[i]/df.shape[0]\n",
    "\n",
    "        final_prob = {}\n",
    "        for i in prob_dict:\n",
    "            temp_dict1 = {}\n",
    "            for j in range((df.shape[1]-1)):\n",
    "                temp_dict2 = {}\n",
    "                temp_list = (np.unique(df.iloc[:,j].values))\n",
    "                for k in temp_list:\n",
    "                    prob = len(df.loc[(df[j]==k) & (df[df.shape[1]-1]==i)])/len(df.loc[df[df.shape[1]-1]==i])\n",
    "                    temp_dict2[k] = prob\n",
    "                temp_dict1[j] = temp_dict2\n",
    "            final_prob[i] = temp_dict1\n",
    "\n",
    "        X_map = []\n",
    "        for i in X:\n",
    "            for j in items_list:\n",
    "                if(i in items_list[j]):\n",
    "                    X_map.append(j)\n",
    "                    break\n",
    "\n",
    "        for i in prob_dict:\n",
    "            print(\"Probability of X belonging to Class \"+str(i)+\":\")\n",
    "            prob = prob_dict[i]\n",
    "            for j in range(len(X)):\n",
    "                item = X[j]\n",
    "                item_index = X_map[j]\n",
    "                prob *= final_prob[i][item_index][item]\n",
    "            print(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ->  0.6595744680851063\n",
      "Precision ->  0.6\n",
      "Recall ->  0.7142857142857143\n",
      "F1_measure ->  0.6521739130434783\n",
      "----------------------------------------------------------------\n",
      "Accuracy ->  0.7446808510638298\n",
      "Precision ->  0.5238095238095238\n",
      "Recall ->  0.8461538461538461\n",
      "F1_measure ->  0.6470588235294118\n",
      "----------------------------------------------------------------\n",
      "Accuracy ->  0.782608695652174\n",
      "Precision ->  0.7647058823529411\n",
      "Recall ->  0.6842105263157895\n",
      "F1_measure ->  0.7222222222222222\n",
      "----------------------------------------------------------------\n",
      "Accuracy ->  0.6739130434782609\n",
      "Precision ->  0.5\n",
      "Recall ->  0.6\n",
      "F1_measure ->  0.5454545454545454\n",
      "----------------------------------------------------------------\n",
      "Accuracy ->  0.6304347826086957\n",
      "Precision ->  0.6\n",
      "Recall ->  0.45\n",
      "F1_measure ->  0.5142857142857142\n",
      "----------------------------------------------------------------\n",
      "Accuracy ->  0.6304347826086957\n",
      "Precision ->  0.5263157894736842\n",
      "Recall ->  0.5555555555555556\n",
      "F1_measure ->  0.5405405405405406\n",
      "----------------------------------------------------------------\n",
      "Accuracy ->  0.8478260869565217\n",
      "Precision ->  0.75\n",
      "Recall ->  0.6923076923076923\n",
      "F1_measure ->  0.72\n",
      "----------------------------------------------------------------\n",
      "Accuracy ->  0.7608695652173914\n",
      "Precision ->  0.5\n",
      "Recall ->  0.5454545454545454\n",
      "F1_measure ->  0.5217391304347826\n",
      "----------------------------------------------------------------\n",
      "Accuracy ->  0.6086956521739131\n",
      "Precision ->  0.4444444444444444\n",
      "Recall ->  0.5\n",
      "F1_measure ->  0.47058823529411764\n",
      "----------------------------------------------------------------\n",
      "Accuracy ->  0.6956521739130435\n",
      "Precision ->  0.5\n",
      "Recall ->  0.5714285714285714\n",
      "F1_measure ->  0.5333333333333333\n",
      "----------------------------------------------------------------\n",
      "Average Accuracy ->  0.7034690101757631\n",
      "Average Precision ->  0.5709275640080593\n",
      "Average Recall ->  0.6159396451501715\n",
      "Average F_measure ->  0.5867396458138145\n"
     ]
    }
   ],
   "source": [
    "if(sample_need == \"y\"):\n",
    "    demo_nb(upd_sample)\n",
    "else:\n",
    "\n",
    "    with open(filePath) as line1:\n",
    "        lines=[line.split(\"\\t\") for line in line1]\n",
    "\n",
    "    data = np.asarray(lines)\n",
    "    sample_data = data[0]\n",
    "    categorical_cols = []\n",
    "\n",
    "    for col in range(sample_data.size):\n",
    "        feature = sample_data[col]\n",
    "        if feature.isalpha():\n",
    "            ft_array,indices = np.unique(data[:,col],return_inverse = True)\n",
    "            data[:,col] = indices.astype(np.float)\n",
    "            categorical_cols.append(col)\n",
    "\n",
    "\n",
    "    # Easier to leave categorical data out for normalization with matrix!\n",
    "    dataset = np.matrix(data[:,:-1],dtype=float,copy=False)\n",
    "    for col in range(dataset.shape[1]):\n",
    "        if col not in categorical_cols:\n",
    "            temp = dataset[:,col]\n",
    "            dataset[:,col] = (temp - temp.min()) / (temp.max() - temp.min())\n",
    "\n",
    "\n",
    "    truth_labels = np.asarray(data[:,-1],dtype = int)\n",
    "    data_subset = np.array_split(dataset,10)\n",
    "    truth_labels_subset = np.array_split(truth_labels,10)\n",
    "\n",
    "\n",
    "    accuracy_average = 0\n",
    "    precision_average = 0\n",
    "    recall_average = 0\n",
    "    f1_Measure_average = 0\n",
    "\n",
    "\n",
    "    def performance_metric(actual, prediction):\n",
    "\n",
    "        a = 0\n",
    "        b = 0\n",
    "        c = 0\n",
    "        d = 0\n",
    "\n",
    "        accuracy = 0\n",
    "        precision = 0\n",
    "        recall = 0\n",
    "        f1_measure = 0\n",
    "\n",
    "        for i in range(len(prediction)):\n",
    "            if actual[i] == 1 and prediction[i] == 1:\n",
    "                a += 1\n",
    "            elif actual[i] == 1 and prediction[i] == 0:\n",
    "                b += 1\n",
    "            elif actual[i] == 0 and prediction[i] == 1:\n",
    "                c += 1\n",
    "            elif actual[i] == 0 and prediction[i] == 0:\n",
    "                d += 1\n",
    "\n",
    "        accuracy += (float(a+d)/(a+b+c+d))\n",
    "\n",
    "        if(a+c != 0):\n",
    "            precision += (float(a)/(a+c))\n",
    "\n",
    "        if(a+b != 0):\n",
    "            recall += (float(a)/(a+b))\n",
    "\n",
    "        f1_measure += (float(2*a)/((2*a)+b+c))\n",
    "\n",
    "        return accuracy, precision, recall, f1_measure\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for subs in range(10):\n",
    "        # We have combined all the training data  using vstack. Not exactly all the training data, we have select 90% training data.\n",
    "        # The rest 10% are in the testing_data\n",
    "\n",
    "        training_data = np.asarray(np.vstack([x for i,x in enumerate(data_subset) if i != subs]))\n",
    "        train_label = np.asarray(np.concatenate([x for i,x in enumerate(truth_labels_subset) if i != subs]))\n",
    "\n",
    "        testing_data = np.asarray(data_subset[subs])\n",
    "        actual_label = np.asarray(truth_labels_subset[subs])\n",
    "\n",
    "        # Over here we are fetching just the continuous data so that we can get the Gaussian Probability here\n",
    "        continuous_traindata = np.delete(training_data,categorical_cols,axis=1)\n",
    "        continuous_testdata = np.delete(testing_data,categorical_cols,axis=1)\n",
    "\n",
    "\n",
    "        # Continuous Data Gaussian\n",
    "        mean_0 = np.mean(continuous_traindata[train_label == 0.0],axis=0)\n",
    "        std_0 = np.std(continuous_traindata[train_label == 0.0], axis=0)\n",
    "\n",
    "        c0_result = norm.pdf(continuous_testdata, mean_0, std_0)\n",
    "\n",
    "        mean_1 = np.mean(continuous_traindata[train_label == 1.0],axis=0)\n",
    "        std_1 = np.std(continuous_traindata[train_label == 1.0], axis=0)\n",
    "\n",
    "        c1_result = norm.pdf(continuous_testdata, mean_1, std_1)\n",
    "\n",
    "\n",
    "        final_0_resultpdf = []\n",
    "        final_1_resultpdf = []\n",
    "\n",
    "        for i2 in range(len(testing_data)):\n",
    "            final_0_resultpdf.append(np.prod(c0_result[i2]))\n",
    "            final_1_resultpdf.append(np.prod(c1_result[i2]))\n",
    "\n",
    "\n",
    "        categorical_0 =  np.ones((len(testing_data)))\n",
    "        categorical_1 = np.ones((len(testing_data)))\n",
    "\n",
    "        for i in range(len(categorical_cols)):\n",
    "\n",
    "            tcat_data = training_data[:,categorical_cols[i]]\n",
    "            testcat_data = testing_data[:,categorical_cols[i]]\n",
    "\n",
    "            tr0_categorical = tcat_data[train_label == 0.0]\n",
    "            tr1_categorical = tcat_data[train_label == 1.0]\n",
    "\n",
    "            pb_0 = len(tr0_categorical)/len(tcat_data)\n",
    "            pb_1 = len(tr1_categorical)/len(tcat_data)\n",
    "\n",
    "            for j in range(len(testcat_data)):\n",
    "\n",
    "                pbx_0 = len(tr0_categorical[np.where((tr0_categorical ==testcat_data[j]))])/len(tr0_categorical)\n",
    "                pbx_1 = len(tr1_categorical[np.where((tr1_categorical ==testcat_data[j]))])/len(tr1_categorical)\n",
    "\n",
    "                if pbx_0 == 0 or pbx_0 == 1:\n",
    "                    pbx_0 = (len(tr0_categorical[np.where((tr0_categorical ==testcat_data[j]))]  )) /(len(tr0_categorical) + len(set(tcat_data))) #Laplace\n",
    "\n",
    "\n",
    "                if pbx_1 == 0 or pbx_1 == 1:\n",
    "                    pbx_1 = (len(tr1_categorical[np.where((tr1_categorical ==testcat_data[j]))])) /(len(tr1_categorical) + len(set(tcat_data)))\n",
    "\n",
    "                pb0_x = (pbx_0 * pb_0)  / len(tcat_data[tcat_data==testcat_data[j]])/len(tcat_data)\n",
    "                pb1_x = (pbx_1 * pb_1) / len(tcat_data[tcat_data==testcat_data[j]])/len(tcat_data)\n",
    "\n",
    "                categorical_0[j] *= (pb0_x)\n",
    "\n",
    "                categorical_1[j] *= (pb1_x)\n",
    "\n",
    "        #now multiply zero and one prob of continuous and cat data\n",
    "\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "        for k in range(len(testing_data)):\n",
    "            # Now we are combining the categorical and continuous variable probabilities!!!\n",
    "\n",
    "            if (categorical_0[k] * final_0_resultpdf[k]) > (categorical_1[k] * final_1_resultpdf[k]):\n",
    "                predictions.append(0)\n",
    "            else:\n",
    "                predictions.append(1)\n",
    "\n",
    "        accuracy, precision, recall, f1_measure = performance_metric(actual_label, predictions)\n",
    "\n",
    "        print(\"Accuracy -> \",accuracy)\n",
    "        accuracy_average += accuracy\n",
    "\n",
    "        print(\"Precision -> \",precision)\n",
    "        precision_average += precision\n",
    "\n",
    "        print(\"Recall -> \",recall)\n",
    "        recall_average += recall\n",
    "\n",
    "        print(\"F1_measure -> \",f1_measure)\n",
    "        f1_Measure_average += f1_measure\n",
    "        print(\"----------------------------------------------------------------\")\n",
    "\n",
    "    print(\"Average Accuracy -> \",accuracy_average/10)\n",
    "    print(\"Average Precision -> \",precision_average/10)\n",
    "    print(\"Average Recall -> \",recall_average/10)\n",
    "    print(\"Average F_measure -> \",f1_Measure_average/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
