{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USER INPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will you be entering:\n",
      "(a)Only Train dataset (1) or\n",
      "(b)Train and Test dataset (2) 1\n",
      "Please enter file name: data1.txt\n",
      "Enter the Number of Neighbors (K) for KNN: 15\n"
     ]
    }
   ],
   "source": [
    "# Enter 1 if providing train and test dataset and 2 if providing only train dataset\n",
    "files = int(input(\"Will you be entering:\\n(a)Only Train dataset (1) or\\n(b)Train and Test dataset (2) \"))\n",
    "train_path = \"\"\n",
    "test_path = \"\"\n",
    "filePath = \"\"\n",
    "if(files == 1):\n",
    "    filePath = input(\"Please enter file name: \")\n",
    "elif(files == 2):\n",
    "    train_path = input(\"Please enter train data file name: \")\n",
    "    test_path = input(\"Please enter test data file name: \")\n",
    "\n",
    "\n",
    "knn = int(input(\"Enter the Number of Neighbors (K) for KNN: \"))\n",
    "fold_count = 1; normalize = \"n\";\n",
    "if(files == 1):\n",
    "    normalize = \"y\" #input(\"Would you like to normalize the dataset (Y/N)? -> \").lower()\n",
    "    kfold = \"y\" #input(\"Would you like to implement Cross Validation (Y/N)? -> \").lower()\n",
    "    if(kfold == \"y\"):\n",
    "        fold_count = 10 #int(input(\"Please enter the number of fold counts: \"))\n",
    "    elif(kfold == \"n\"):\n",
    "        fold_count = 1\n",
    "    else:\n",
    "        print(\"This value should be an integer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurecheck(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def get_fold_range(row_count,fold_count):\n",
    "    k_main = []\n",
    "    if(fold_count==1):\n",
    "        step = round(0.23*row_count)\n",
    "        temp = []\n",
    "        temp.append(0)\n",
    "        temp.append(step)\n",
    "        k_main.append(temp)\n",
    "    else:\n",
    "        step = math.floor(row_count / fold_count)\n",
    "        rem = row_count - fold_count * step\n",
    "        p = 0\n",
    "        \n",
    "        for i in range(fold_count):\n",
    "            val_add = 0\n",
    "            if(rem > 0):\n",
    "                val_add = 1\n",
    "            temp = []\n",
    "            temp.append(p)\n",
    "            p = p + step + val_add\n",
    "            temp.append(p)\n",
    "            k_main.append(temp)\n",
    "            rem -= 1\n",
    "        #print(k_main)\n",
    "    return(k_main)\n",
    "\n",
    "def performance_calculation(prediction,actual):\n",
    "    a = 0; b = 0; c = 0; d = 0;\n",
    "    for i in range(len(prediction)):\n",
    "        if((prediction[i] == 1) and (actual[i] == 1)):\n",
    "            a += 1\n",
    "        elif((prediction[i] == 0) and (actual[i] == 1)):\n",
    "            b += 1\n",
    "        elif((prediction[i] == 1) and (actual[i] == 0)):\n",
    "            c += 1\n",
    "        elif((prediction[i] == 0) and (actual[i] == 0)):\n",
    "            d += 1\n",
    "    return(a,b,c,d)\n",
    "\n",
    "def normalizing_dataset(dataset):\n",
    "    df11 = pd.DataFrame(dataset)\n",
    "    #data_stats = [df11.mean().tolist(), df11.std().tolist()]\n",
    "    data_stats = [df11.min().tolist(), df11.max().tolist()]\n",
    "    #pd.DataFrame(data_stats)\n",
    "    for z6 in range(len(dataset)):\n",
    "        for z7 in range(len(dataset[z6])):\n",
    "            dataset[z6][z7] = (dataset[z6][z7] - data_stats[0][z7])/(data_stats[1][z7]-data_stats[0][z7])\n",
    "            #dataset[z6][z7] = (dataset[z6][z7] - data_stats[0][z7])/(data_stats[1][z7])\n",
    "    return dataset\n",
    "\n",
    "def check_sep(line):\n",
    "    flag = \"\"\n",
    "    if(\" \" in line):\n",
    "        flag = \" \"\n",
    "    elif(\"\\t\" in line):\n",
    "        flag = \"\\t\"\n",
    "    else:\n",
    "        print(\"Data should be sperated by \\t or single space!\")\n",
    "    return flag\n",
    "\n",
    "def dist_calc(c):\n",
    "    sum1 = 0\n",
    "    for s in c:\n",
    "        sum1 += (s**2)\n",
    "    sum1 = sum1**(1/2)\n",
    "    return sum1\n",
    "\n",
    "# MAIN KNN CODE\n",
    "def main_knn(files, k_main, dataset, unique_labels, knn, df):\n",
    "    accuracy_list = []; precision_list = []; recall_list = []; fmeasure_list = [];\n",
    "\n",
    "    for i in k_main:\n",
    "        train_data = {}; test_data = {}; pred_labels = {}; dist = {}; label_count = {};\n",
    "\n",
    "        for j in range(row_count):\n",
    "            if(j in range(i[0],i[1])):\n",
    "                test_data[j] = dataset[j]\n",
    "                pred_labels[j] = -999\n",
    "            else:\n",
    "                train_data[j] = dataset[j]\n",
    "\n",
    "        for j2 in test_data.keys():\n",
    "            d1 = test_data[j2]\n",
    "\n",
    "            for l1 in unique_labels:\n",
    "                label_count[l1] = 0\n",
    "\n",
    "            for k in train_data.keys():\n",
    "                d2 = train_data[k]\n",
    "                c = np.array(d2) - np.array(d1)\n",
    "                dist[k] = round(dist_calc(c),4)\n",
    "\n",
    "            dd = dict(OrderedDict(sorted(dist.items(), key=lambda x: x[1]))) #\n",
    "            k_vals = (list(dd.keys()))[0:knn]\n",
    "            #print(k_vals)\n",
    "            for sn1 in range(knn):\n",
    "                temp_label = true_label[k_vals[sn1]]\n",
    "                #print(temp_label)\n",
    "                for l2 in unique_labels:\n",
    "                    if(temp_label==l2):\n",
    "                        label_count[l2] = label_count[l2] + 1\n",
    "            #print(label_count)\n",
    "            lab1 = -999; lab2 = -999;\n",
    "            for (z8,z9) in label_count.items():\n",
    "                if(z9>lab2):\n",
    "                    lab1 = z8\n",
    "                    lab2 = z9\n",
    "            #print(str(lab1)+\" --- \"+str(lab2))\n",
    "            pred_labels[j2] = lab1\n",
    "\n",
    "        comp = list(pred_labels.values())\n",
    "        fin = list(df.iloc[i[0]:i[1],df.shape[1]-1].values)\n",
    "        (a,b,c,d) = performance_calculation(comp,fin)\n",
    "        accuracy = (a+d)/(a+b+c+d)\n",
    "        precision = a/(a+c)\n",
    "        recall = a/(a+b)\n",
    "        fmeasure = (2*a)/((2*a)+b+c)\n",
    "        accuracy_list.append(accuracy); precision_list.append(precision);\n",
    "        recall_list.append(recall); fmeasure_list.append(fmeasure);\n",
    "        \n",
    "    return(accuracy_list, precision_list, recall_list, fmeasure_list)\n",
    "\n",
    "def display_measures(acc, prec, rec, fm):\n",
    "    if(len(acc)==1):\n",
    "        print(\"Algorithm Measures\")\n",
    "        print(\"Accuracy -> \"+str(acc[0]))\n",
    "        print(\"Precision -> \"+str(prec[0]))\n",
    "        print(\"Recall -> \"+str(rec[0]))\n",
    "        print(\"F-Measure -> \"+str(fm[0]))\n",
    "    else:\n",
    "        avg_accuracy = 0; avg_precision = 0; avg_recall = 0; avg_fmeasure = 0;\n",
    "        for i in range(len(acc)):\n",
    "            print(\"----------------------------------------\")\n",
    "            print(\"Cross Validation Iteration -> \"+str(i+1))\n",
    "            print(\"Algorithm Measures\")\n",
    "            print(\"Accuracy -> \"+str(acc[i]))\n",
    "            print(\"Precision -> \"+str(prec[i]))\n",
    "            print(\"Recall -> \"+str(rec[i]))\n",
    "            print(\"F-Measure -> \"+str(fm[i]))\n",
    "\n",
    "            avg_accuracy += acc[i]; avg_precision += prec[i]; avg_recall += rec[i]; avg_fmeasure += fm[i];\n",
    "\n",
    "        print(\"----------------------------------------\")\n",
    "        print(\"Average Measures\")\n",
    "        print(\"Average Accuracy -> \"+str(avg_accuracy/len(acc)))\n",
    "        print(\"Average Precision -> \"+str(avg_precision/len(prec)))\n",
    "        print(\"Average Recall -> \"+str(avg_recall/len(rec)))\n",
    "        print(\"Average F-Measure -> \"+str(avg_fmeasure/len(fm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = \"\"; df1 = \"\"; file2 = \"\"; df2 = \"\"; df = \"\";\n",
    "row_count = 0; dataset = []; full_dataset = []; \n",
    "k_main = []; true_label = {}; update_data = {}; unique_labels = []; \n",
    "\n",
    "if(files==2):\n",
    "    file1 = open(train_path, \"r\")\n",
    "    file2 = open(test_path, \"r\")\n",
    "    lines = file1.readlines()\n",
    "    lines2 = file2.readlines()\n",
    "    flag11 = 2; flag12 = 2; #Line Seperator\n",
    "    flag21 = flag22 = 2; #Chosing method to handle nominal values\n",
    "    z1 = 0; temp_upd_count = -1;\n",
    "\n",
    "    for line in lines:\n",
    "        flag11 = check_sep(line)\n",
    "        break\n",
    "    for line in lines2:\n",
    "        flag12 = check_sep(line)\n",
    "        break\n",
    "\n",
    "    for line in lines:\n",
    "        data = line.strip().split(flag11)\n",
    "        if(flag21 == 1):\n",
    "            for i in data:\n",
    "                if(featurecheck(i)==False):\n",
    "                    data.remove(i)\n",
    "        elif(flag21 == 2):\n",
    "            for i in range(len(data)):\n",
    "                if(featurecheck(data[i])==False):\n",
    "                    if(data[i] in update_data.keys()):\n",
    "                        data[i] = update_data[data[i]]\n",
    "                    else:\n",
    "                        temp_upd_count += 1\n",
    "                        data[i] = temp_upd_count\n",
    "\n",
    "        data = [float(ele) for ele in data]\n",
    "        true_label[z1] = data[-1]\n",
    "        z1 += 1\n",
    "        dataset.append(data[:-1])\n",
    "        full_dataset.append(data[:])\n",
    "\n",
    "    for line in lines2:\n",
    "        data = line.strip().split(flag12)\n",
    "        if(flag22 == 1):\n",
    "            for i in data:\n",
    "                if(featurecheck(i)==False):\n",
    "                    data.remove(i)\n",
    "        elif(flag22 == 2):\n",
    "            for i in range(len(data)):\n",
    "                if(featurecheck(data[i])==False):\n",
    "                    if(data[i] in update_data.keys()):\n",
    "                        data[i] = update_data[data[i]]\n",
    "                    else:\n",
    "                        temp_upd_count += 1\n",
    "                        data[i] = temp_upd_count\n",
    "\n",
    "        data = [float(ele) for ele in data]\n",
    "        true_label[z1] = data[-1]\n",
    "        z1 += 1\n",
    "        dataset.append(data[:-1])\n",
    "        full_dataset.append(data[:])\n",
    "\n",
    "    df1 = pd.read_csv(train_path, sep = flag11, lineterminator = '\\n', header = None)\n",
    "    df2 = pd.read_csv(test_path, sep = flag11, lineterminator = '\\n', header = None)\n",
    "    unique_labels = list(set(true_label.values()))\n",
    "    df = pd.DataFrame(full_dataset)\n",
    "    \n",
    "    row_count = df.shape[0]\n",
    "    k_main.append([df1.shape[0],len(dataset)])\n",
    "    #(unique_labels,b) = np.unique(df1.iloc[:,df1.shape[1]].values,return_counts = True)\n",
    "else:\n",
    "    file = open(filePath, \"r\")\n",
    "    lines = file.readlines()\n",
    "    flag1 = 2 #Line Seperator\n",
    "    flag2 = 2 #Chosing method to handle nominal values\n",
    "    z1 = 0; temp_upd_count = -1;\n",
    "\n",
    "    for line in lines:\n",
    "        if(\" \" in line):\n",
    "            flag1 = \" \"\n",
    "        elif(\"\\t\" in line):\n",
    "            flag1 = \"\\t\"\n",
    "        else:\n",
    "            print(\"Data should be sperated by \\t or single space!\")\n",
    "        break\n",
    "\n",
    "    for line in lines:\n",
    "        data = line.strip().split(flag1)\n",
    "        if(flag2 == 1):\n",
    "            for i in data:\n",
    "                if(featurecheck(i)==False):\n",
    "                    data.remove(i)\n",
    "        elif(flag2 == 2):\n",
    "            for i in range(len(data)):\n",
    "                if(featurecheck(data[i])==False):\n",
    "                    if(data[i] in update_data.keys()):\n",
    "                        data[i] = update_data[data[i]]\n",
    "                    else:\n",
    "                        temp_upd_count += 1\n",
    "                        data[i] = temp_upd_count\n",
    "        data = [float(ele) for ele in data]\n",
    "        true_label[z1] = data[-1]\n",
    "        z1 += 1\n",
    "        dataset.append(data[:-1])\n",
    "\n",
    "    df = pd.read_csv(filePath, sep = flag1, lineterminator = '\\n', header = None)\n",
    "    if(normalize == \"y\"):\n",
    "        dataset = normalizing_dataset(dataset)\n",
    "    row_count = df.shape[0]\n",
    "    col_count = df.shape[1]-1\n",
    "    k_main = get_fold_range(row_count,fold_count)\n",
    "    unique_labels = list(set(true_label.values()))\n",
    "    #(unique_labels,b) = np.unique(df1.iloc[:,col_count+1].values,return_counts = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Cross Validation Iteration -> 1\n",
      "Algorithm Measures\n",
      "Accuracy -> 0.9649122807017544\n",
      "Precision -> 1.0\n",
      "Recall -> 0.9130434782608695\n",
      "F-Measure -> 0.9545454545454546\n",
      "----------------------------------------\n",
      "Cross Validation Iteration -> 2\n",
      "Algorithm Measures\n",
      "Accuracy -> 0.9473684210526315\n",
      "Precision -> 0.9375\n",
      "Recall -> 0.8823529411764706\n",
      "F-Measure -> 0.9090909090909091\n",
      "----------------------------------------\n",
      "Cross Validation Iteration -> 3\n",
      "Algorithm Measures\n",
      "Accuracy -> 0.9649122807017544\n",
      "Precision -> 1.0\n",
      "Recall -> 0.8571428571428571\n",
      "F-Measure -> 0.9230769230769231\n",
      "----------------------------------------\n",
      "Cross Validation Iteration -> 4\n",
      "Algorithm Measures\n",
      "Accuracy -> 0.9122807017543859\n",
      "Precision -> 1.0\n",
      "Recall -> 0.7619047619047619\n",
      "F-Measure -> 0.8648648648648649\n",
      "----------------------------------------\n",
      "Cross Validation Iteration -> 5\n",
      "Algorithm Measures\n",
      "Accuracy -> 0.9298245614035088\n",
      "Precision -> 1.0\n",
      "Recall -> 0.8\n",
      "F-Measure -> 0.8888888888888888\n",
      "----------------------------------------\n",
      "Cross Validation Iteration -> 6\n",
      "Algorithm Measures\n",
      "Accuracy -> 0.9473684210526315\n",
      "Precision -> 1.0\n",
      "Recall -> 0.8888888888888888\n",
      "F-Measure -> 0.9411764705882353\n",
      "----------------------------------------\n",
      "Cross Validation Iteration -> 7\n",
      "Algorithm Measures\n",
      "Accuracy -> 0.9473684210526315\n",
      "Precision -> 0.9523809523809523\n",
      "Recall -> 0.9090909090909091\n",
      "F-Measure -> 0.9302325581395349\n",
      "----------------------------------------\n",
      "Cross Validation Iteration -> 8\n",
      "Algorithm Measures\n",
      "Accuracy -> 0.9649122807017544\n",
      "Precision -> 0.9333333333333333\n",
      "Recall -> 0.9333333333333333\n",
      "F-Measure -> 0.9333333333333333\n",
      "----------------------------------------\n",
      "Cross Validation Iteration -> 9\n",
      "Algorithm Measures\n",
      "Accuracy -> 0.9122807017543859\n",
      "Precision -> 1.0\n",
      "Recall -> 0.8387096774193549\n",
      "F-Measure -> 0.9122807017543859\n",
      "----------------------------------------\n",
      "Cross Validation Iteration -> 10\n",
      "Algorithm Measures\n",
      "Accuracy -> 0.9285714285714286\n",
      "Precision -> 1.0\n",
      "Recall -> 0.8181818181818182\n",
      "F-Measure -> 0.9\n",
      "----------------------------------------\n",
      "Average Measures\n",
      "Average Accuracy -> 0.9419799498746867\n",
      "Average Precision -> 0.9823214285714286\n",
      "Average Recall -> 0.8602648665399265\n",
      "Average F-Measure -> 0.9157490104282531\n"
     ]
    }
   ],
   "source": [
    "(acc, prec, rec, fm) = main_knn(files, k_main, dataset, unique_labels, knn, df)\n",
    "\n",
    "display_measures(acc, prec, rec, fm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
